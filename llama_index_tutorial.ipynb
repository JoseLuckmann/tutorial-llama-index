{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SQLDatabase\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurando parametros com relação ao modelo LLM utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-SUA+CHAVE\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "chunk_size = 2048\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "Settings.chunk_size = chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conectamos ao nosso banco de dados, para fins de estudos usaremos o SQLite mas esta conexão pode ser substituida por qualquer conexão usando o SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-08 11:14:07--  https://github.com/JoseLuckmann/tutorial-llama-index/raw/main/rh.db\n",
      "Resolving github.com (github.com)... 20.201.28.151\n",
      "Connecting to github.com (github.com)|20.201.28.151|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/JoseLuckmann/tutorial-llama-index/main/rh.db [following]\n",
      "--2024-04-08 11:14:08--  https://raw.githubusercontent.com/JoseLuckmann/tutorial-llama-index/main/rh.db\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 176128 (172K) [application/octet-stream]\n",
      "Saving to: ‘rh.db’\n",
      "\n",
      "rh.db               100%[===================>] 172.00K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2024-04-08 11:14:08 (2.27 MB/s) - ‘rh.db’ saved [176128/176128]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/JoseLuckmann/tutorial-llama-index/raw/main/rh.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"sqlite:///rh.db\"\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora conectamos o nosso banco de dados ao Index aqui surgem alguns objetos novos que é importante entendermos\n",
    "\n",
    "- SQLDatabase: Uma abstração simples para permitir a conexão entre o SQLAlchemy e o LLamaIndex\n",
    "- NLSQLTableQueryEngine: Motor de consulta de tabelas SQL em linguagem natural\n",
    "- QueryEngineTool: Uma ferramenta que utiliza um motor de pesquisa (no nosso caso a NLSQLTable)\n",
    "- SubQuestionQueryEngine? Divide uma consulta complexa em muitas sub-perguntas e no seu motor de consulta de destino para execução. Após a execução de todas as sub-perguntas, todas as respostas são reunidas e enviadas para o sintetizador de respostas para produzir a resposta final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_database = SQLDatabase(engine)\n",
    "sql_query_engine = NLSQLTableQueryEngine(sql_database=sql_database, verbose=True)\n",
    "sql_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=sql_query_engine,\n",
    "    description=\"\"\"Utilizado para traduzir a linguagem natural em linguagem SQL para executar as querys\n",
    "Base de dados de recursos humanos com informações de funcionarios, salarios, vagas e etc\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "        query_engine_tools=[sql_tool], #No nosso caso temos apenas uma conexão, mas aqui é possivel informar varias conexões\n",
    "    )\n",
    "\n",
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"sub_question_query_engine\",\n",
    "        description=\"Util para responder perguntas sobre as bases de dados que você tem acesso.\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "tools = [query_engine_tool] + [sql_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos um agente da OpenAI com acesso as nossas ferramentas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OpenAIAgent.from_tools(tools, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar algumas perguntas agora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Você tem um total de 1000 funcionários."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = agent.chat(\"Quantos funcionários eu tenho?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "O funcionário com o maior salário é Emily Lewis, com um salário de $9986.00."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = agent.chat(\"Quem é o funcionário com maior salário?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Aqui está a tabela com a soma da folha salarial mês a mês dos últimos 6 meses:\n",
       "\n",
       "- Janeiro: $498,353.00\n",
       "- Fevereiro: $570,587.00\n",
       "- Março: $507,059.00\n",
       "- Abril: $422,937.00\n",
       "- Maio: $464,552.00\n",
       "- Junho: $560,706.00"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = agent.chat(\"Crie uma tabela com a soma da minha folha salarial mes a mes dos ultimos 6 meses\")\n",
    "display(Markdown(str(response)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
